encoder:
  feat_in: 80
  feat_out: -1 # you may set it if you need different output size other than the default d_model
  n_layers: 16
  d_model: &d_model 256

  # Sub-sampling params
  subsampling: striding # vggnet or striding, vggnet may give better results but needs more memory
  subsampling_factor: 4 # must be power of 2
  # TODO
  # Setting -1 does not working
  # May be need to set d_model from reference
  subsampling_conv_channels: 256 # -1 sets it to d_model
  
  # Feed forward module's params
  ff_expansion_factor: 4

  # Multi-headed Attention Module's params
  self_attention_model: rel_pos # rel_pos or abs_pos
  n_heads: 4 # may need to be lower for smaller d_models
  # [left, right] specifies the number of steps to be seen from left and right of each step in self-attention
  att_context_size: [-1, -1] # -1 means unlimited context
  xscaling: true # scales up the input embeddings by sqrt(d_model)
  untie_biases: true # unties the biases of the TransformerXL layers
  pos_emb_max_len: 5000

  # Convolution module's params
  conv_kernel_size: 31

  ### regularization
  dropout: 0.1 # The dropout used in most of the Conformer Modules
  dropout_emb: 0.0 # The dropout used for embeddings
  dropout_att: 0.1 # The dropout for multi-headed attention modules
